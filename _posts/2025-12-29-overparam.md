---
layout: distill
title: Understanding Overparameterisation and Double Descent
# description: an example of a distill-style blog post and main elements
tags: distill formatting
giscus_comments: true
date: 2025-03-01
featured: true

authors:
  - name: Ryan Singh

bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
# toc:
#   - name: Equations
# if a section has subsections, you can add them as follows:
# subsections:
#   - name: Example Child Subsection 1
#   - name: Example Child Subsection 2

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## What is overparameterisation?

At an intuitive level machine learning researchers say a model is overparameterised if it has the capacity for perfectly fitting the training data. Crucially, the empirical risk (or loss) is minimised by a solution which performs perfectly on the training set but poorly on the test set. This is reflected by the classic bias-variance trade-off a staple of all machine learning 101 courses.
<aside><p> Any solution achieving zero empirical risk is necessarily capturing noise - if noise is present in the data generation.</p> </aside>
However, we remember, there could be many zero-loss solutions and some of these might be preferable to others. If there are many solutions, the solution chosen is a function of the optimiser used. The suprising finding is:
* The somewhat natural solution (naturally found by gradient descent) has a simplicity bias.
* The performance will improve with *more* parameters.

Why was this a suprise? Probably due to the amount we have drilled into us that we should be extremely careful about the number of parameters we use to fit data, and suddenly we are told more parameters is better!

---

As a running example we can consider the linear regression problem generated by:
\begin{equation}
    y = \theta \cdot x + \epsilon
\end{equation}
for $x \sim N(0, I_{d\times d})$ and $\epsilon \sim N(0, \sigma^2)$. Along with a model of the form:
\begin{equation}
    \hat{y} = \hat{\theta}\cdot x + \epsilon
\end{equation}
$\hat{\theta} \in \mathbb{R}^p$ for some $p\leq d$.

$\newcommand\DataX{\bf{X}}$ $\newcommand\DataY{\bf{y}}$

The empirical risk/ loss/ likelihood of the data is given by:
\begin{equation}
    L(\hat{\theta}) = \lvert\lvert \DataY - \DataX\hat{\theta} \mid\mid^2_2
\end{equation}

---
## Ill-posed inverse problems

We can roughly understand the behaviour by considering the behaviour of the linear set of equations:
\begin{equation}
    \DataY = \DataX\hat{\theta} 
\end{equation}
because if we can solve this equation we will have the set of maximum likelihood estimators. Obviously this is not quite right in the underparameterised regime.

It is easy to see that when $d > n$ the maximum likelihood problem is ill-posed, there are more uknowns than equations. Alternatively phrased the linear map $X: \mathbb{R}^{d} \rightarrow \mathbb{R}^n$ has kernel size $d-n$ by rank-nullity. Therefore any solution $\hat{\theta}$ has the same empirical loss as $\hat{\theta} + k$. i.e. the parameter space has a massive space of symmetries, or redundancies, with respect to the data.

If we squint we can see why the test loss spikes when $n=d$. Effectively we are trying to find a left inverse for $X$ mapping the target-space to the parameter-space. In the square case the solution is uniquely $\hat{\theta}=X^{-1}y$. The variance part (of the bias variance decomposition) measures how sensitive this estimator is to a perturbation. Intuitively, if the number of uknowns is exactly the same as the number of equations the retrieved uknown is going to maximally depend on the equations.

<!-- More formally we can plug in $y' = y + \epsilon$ to find $\hat{\theta} - \hat{\theta}' = X^{-1}\epsilon$. -->
<!-- We immediately see that small eigenvalues of $X$ are going to be problematic.   -->
### What solution does GD find?
By a Noether type argument, we know that this symmetry corresponds to a conserved quantity - here trivially the location in the kernel space. The argument goes that if GD is initalised at 0, the solution found will be the one with $k=0$. It is easy to see by that this is also the solution with minimum norm.

### What's so good about the minimum norm solution?

On it's own nothing, given our data and assumed likelihood every solution is equivalent.

The minimum norm solution can be seen as the limit of ridge regression:
\begin{equation}
  \hat{\theta} = \lim_{\lambda \rightarrow 0} \theta_{\lambda}
\end{equation}
\begin{equation}
  \theta_{\lambda} = \min \ \lvert\lvert \DataY - \DataX\hat{\theta} \rvert\rvert^2_2 + \lambda \lvert\lvert  \theta \rvert\rvert^2_2
\end{equation}
So we are really just hand-picking the solution which solves the equation while having minimum l2-norm. Alternatively we are looking for the limit of proper Bayes-Estimators. 

<!-- In general it turns out the spectrum has isolated eigenvalues at zero and the essential spectrum seperated away from it. A crucial property of the essential spectrum is that it is closed under compact pertubations $\sigma(X+E) = \sigma(X)$ -->

<!-- 
Suppose we see a test data point that is close to a training data point $x = x_{tr} + \epsilon$ then the prediction $\hat{y} = y_{tr} + \hat{\theta}\epsilon$  -->

---
Functionally the estimator has to contain the memorised mapping -- however information theoretically this is invisible (as paramaters go to infinity).

---
Why be overparameterised? to contain the correct solution - to make optimisation easier

If we observed the empirical (quadratic) loss-landscape as $d \rightarrow n$ we would see sharper and sharper minima in practice this means we have to use a smaller and smaller learning rate to gaurantee stability.



---
But hold on, we also know from decision theory a Bayes-rule would make admissable inferences no matter what the ratio hypothesis space size to data points. Even given a huge hypothesis space and a single data point we know no algorithm can universally outperform the MAP estimator (c.f. no free lunch). How do we square this?

One way to think about the overfitting solution is through ill-posedness, to minimise the empricical loss we are really trying to approximately solve the equation:
\begin{equation}
  Y = \hat{\theta}X
\end{equation}
with $Y \in \mathbb{R}^n$, $X \in \mathbb{R}^{d\times n}$, $\hat{\theta} \in \mathbb{R}^{d}$. i.e. solve $d$ uknowns with $n$ equations. Clearly if $d > n$ then the solution is not unique and to resolve this ambiguity we need to place a prior preference over the set of possible solutions. Importantly 

In fact if we truly believe all of the parameters are possible than any prior which places zero mass on some measurable subset is *inadmissable* i.e. exactly what we would do if only considered a $\mathbb{R}^{p} \subset \mathbb{R}^d$ hypothesis space. 


---


---

## Physical Learning Systems

For me specifically, our current theory of physical learning systems is severely underdeveloped. What do I mean a physical learning system? At the coarsest level a physical learning system:

> An automata which improves it's predictions over time and under which not all transformations are possible.

This seems like a natural starting point for understanding any kind of intelligence embodied <d-footnote>all intelligence satisfies this claim</d-footnote> in the real world. So why is it underdeveloped? I would argue this is due to a variety of factors. Firstly the focuses of traditional theory

- The blindspot of Information Theory. _Information theory is incredibly useful at allowing us to abstract away physical details of the system, however it can only supply lower bounds away from a large-sytem limit and assigns no cost to computation_.
- Statistics and Control Theory are dominated by performance gaurantees rather than tradeoffs. _In particular traditional statistical theory does not consider computational costs at all._ <d-footnote> On the one hand this has made the cross-talk between machine learning and theory incredibly challenging </d-footnote>
<!-- * Computational Complexity classes. *Based on a universal model of computation*. -->

Secondly the success of Deep Learning has skewed modern research,

- Exploiting scaling laws. _The discovery of data- compute scaling laws and the availability of big data and compute have been exploited at the expense of ever rising energy cost_. <d-footnote>The extreme pessimist would argue that since the invention of backprop there have been no fundamental breakthroughs in Deep Learning and it's success can mostly be attributed to cheaper compute and larger data.</d-footnote>
- Desire for universal learners. _While the human brain clearly has dedicated circuitry for different types of signal and computation, in machine learning (at the cost of resource efficiency) the universal solution is preferred. Designing inductive biases is seen as an anathema c.f. reward is enough_

Put simply, the most developed frameworks we have are designed to ignore physical implementation (as long as it can be put on a universal turing machine).

Why is it important?

- Neuromorphic advantage. _If we don't want to treat neuromorphic devices as turing machines which simply simulate the ideal learner we need to understand problem dependent design choices._
- Mechanistic interpretability. _There is currently a severe lack of understanding of the solutions that a neural network finds. Attempts to decode activity almost always use_

## A path forward?

What would a theory of physical learning systems look like?

---

One of the first identifiable developments in what we now call AI was the Hopfield Network. The Hopfield Network was directly answering the question of whether a physical system (it's in the name of the paper!<d-footnote>Neural networks and physical systems with emergent collective computational abilities.</d-footnote>) could learn.

---

## How did I get here?

When I started my PhD, I was intrigued that Pyschologists categorise memories into Episodic, Semantic, Procedural etc. While as far as I am aware there is no account for this in the theories of learning. Naturally, I thought incoporating these differences as priors (or inductive biases) would be crucial for desinging better learning algorithms.

However, influenced by the work linking the attention mechanism to Hopfield Networks, my first project was to try and understand the attention mechanism and whether or not there was a link to attention as postulated in predictive coding. Long story short, they are potentially connected if the generative model of agent used a discrete prior over adjacent nodes rather than a continuous prior. This left me wondering if there was some fundamental reason for these discrete priors. Was the switching really a reflection of some property of the world? Perhaps the nature of language data, however vision transformers were already succesful.

Naturally we began to think of the discreteness as a type of coarse-graining, or intentional forgetting about certain parts of the input. Which led to Poppy and I trying to develop a simple control algorithm, which performed long range planning in the discrete domain, while performing low-level continuous control actions at an approximate level. In particular we wanted to exploit the tractability of evaluating information theoretic optimal exploration terms in the smaller discrete representation.

As an aside, I became curious about certain claims in the Active Inference literature about intrinsic motivation and optimal exploration - which I now understand to be misguided. This led to a fundamental understanding that any non-trivial exploration-exploitation problem is computationally intractable, only approximate solutions are allowed. Further I came to understand that the bayesian solutions are precisely ones that don't forget _any_ relevant information from the world.

On the other hand working with Francesco, we had been looking at the theory of predictive coding in linear networks. In such networks it was possible to see explicitly the advantage of iterative inference, since they use more than just the immediate gradient of loss landscape. However we were left wondering whether the time spent performing inference came at exactly the same (or greater) cost as performing multiple gradient steps (I suspect it does). I also started to wonder about whether there are fundamental bounds to different optimisers - a second-optimiser costs more computationally, but allows quicker inference.

Driven by questions about what advantage could be gained by forgetting certain aspects of the world, I began working with Miguel who had been trying to deconstruct Yann Le Cunn's claims about reconstruction. Ultimately we explored a bit of theory which showed that when a JEPA type architecture as enough capacity -- it will converge to the same representation (information theoretically) as an explicit model. With the main of advantage of JEPA as avoiding backpropagating through the explicit model.

---

## Citations

Citations are then used in the article body with the `<d-cite>` tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.

The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.

Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.

---

## Footnotes

Just wrap the text you would like to show up in a footnote in a `<d-footnote>` tag.
The number of the footnote will be automatically generated.

<d-footnote>This will become a hoverable footnote.</d-footnote>
.
